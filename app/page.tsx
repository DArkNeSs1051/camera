"use client";

import React, { useEffect, useRef } from "react";
import * as tf from "@tensorflow/tfjs";
import * as posedetection from "@tensorflow-models/pose-detection";
import "@tensorflow/tfjs-backend-webgl";

export default function Home() {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const detectorRef = useRef<posedetection.PoseDetector | null>(null);

  useEffect(() => {
    const init = async () => {
      try {
        // ðŸ”§ Force WebGL backend à¸žà¸£à¹‰à¸­à¸¡ debug config
        tf.env().set("WEBGL_CPU_FORWARD", false);
        tf.env().set("WEBGL_PACK", true);
        tf.env().set("WEBGL_VERSION", 1); // à¸¥à¸­à¸‡à¹ƒà¸Šà¹‰ WebGL1 à¹à¸—à¸™ WebGL2

        await tf.setBackend("webgl");
        await tf.ready();

        const backend = tf.getBackend();
        console.log("âœ… TF backend in use:", backend);

        // ðŸ” à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² backend à¹ƒà¸Šà¹‰ webgl à¸ˆà¸£à¸´à¸‡à¹„à¸«à¸¡
        if (backend !== "webgl") {
          throw new Error("WebGL backend not active, fallback in progress");
        }

        // âœ… à¸ªà¸£à¹‰à¸²à¸‡ detector
        const detector = await posedetection.createDetector(
          posedetection.SupportedModels.MoveNet,
          {
            modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
          }
        );
        detectorRef.current = detector;

        // âœ… à¹€à¸›à¸´à¸”à¸à¸¥à¹‰à¸­à¸‡
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
        });
        if (videoRef.current) {
          videoRef.current.srcObject = stream;
          videoRef.current.onloadedmetadata = () => {
            videoRef.current?.play();
            detectPose();
          };
        }
      } catch (err) {
        console.warn(
          "âŒ WebGL failed or not supported, switching to CPU...",
          err
        );
        await tf.setBackend("cpu");
        await tf.ready();
        console.log("ðŸ§  TF fallback to:", tf.getBackend());

        alert(
          "âš ï¸ à¸­à¸¸à¸›à¸à¸£à¸“à¹Œà¸™à¸µà¹‰à¹„à¸¡à¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š WebGL à¸ªà¸³à¸«à¸£à¸±à¸š TensorFlow.js\nà¸£à¸°à¸šà¸šà¸ˆà¸°à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹„à¸›à¹ƒà¸Šà¹‰ CPU à¸‹à¸¶à¹ˆà¸‡à¸­à¸²à¸ˆà¸Šà¹‰à¸²à¸¥à¸‡"
        );
      }
    };

    const detectPose = async () => {
      if (!videoRef.current || !canvasRef.current || !detectorRef.current)
        return;

      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext("2d");
      if (!ctx) return;

      const render = async () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;

        const poses = await detectorRef.current!.estimatePoses(video);
        console.log("ðŸŽ¯ poses", poses);

        ctx.save();
        ctx.clearRect(0, 0, canvas.width, canvas.height);
        ctx.scale(-1, 1);
        ctx.translate(-canvas.width, 0);

        poses.forEach((pose) => {
          pose.keypoints.forEach((keypoint) => {
            if (keypoint.score && keypoint.score > 0.2) {
              ctx.beginPath();
              ctx.arc(keypoint.x, keypoint.y, 6, 0, 2 * Math.PI);
              ctx.fillStyle = "#00FF00";
              ctx.fill();
            }
          });
        });

        ctx.restore();
        requestAnimationFrame(render);
      };

      render();
    };

    init();
  }, []);

  return (
    <div className="w-full h-screen flex items-center justify-center bg-black relative">
      <video
        ref={videoRef}
        className="absolute w-full h-full z-0"
        style={{
          width: "100%",
          height: "100%",
          objectFit: "contain",
          transform: "scaleX(-1)",
        }}
        muted
        playsInline
      />
      <canvas
        ref={canvasRef}
        className="absolute w-full h-full z-10 pointer-events-none"
        style={{
          width: "100%",
          height: "100%",
          objectFit: "contain",
        }}
      />
    </div>
  );
}
